{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b2e5d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dfe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------- USER SETTINGS ----------------\n",
    "CSV_FILE = \"../nz_1980_2024_mc.csv\"  \n",
    "OUT_BASE = \"final-data\"                    \n",
    "MC = 3.0                             # Completeness magnitude\n",
    "WINDOW_SIZE = 100                     # Number of events per sliding window\n",
    "STEP = 10                             # Step size for sliding window\n",
    "N_R = 30                              # Number of points in log(r) for C(r)\n",
    "BOOTSTRAP_REPS = 0\n",
    "NUM_WORKERS = max(1, min(4, cpu_count()-1))  # Avoid CPU overload\n",
    "BATCH_SIZE = 1000                     # Number of windows per batch\n",
    "MAX_WINDOWS = 5000                    # Maximum number of windows to process\n",
    "MAX_EVENTS_FOR_CR = 2000              # Max events per C(r) calculation\n",
    "# ------------------------------------------------\n",
    "\n",
    "R_EARTH = 6371.0  # Earth radius in km\n",
    "\n",
    "# ----------------  Functions ----------------\n",
    "def find_column(df, choices):\n",
    "    \"\"\"Find first matching column name from a list of possible choices.\"\"\"\n",
    "    for c in choices:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def safe_great_circle_matrix(lat_deg, lon_deg, max_size=2000):\n",
    "    \"\"\"Compute great-circle distance matrix safely with memory check.\"\"\"\n",
    "    if len(lat_deg) > max_size:\n",
    "        indices = np.random.choice(len(lat_deg), max_size, replace=False)\n",
    "        lat_deg = lat_deg[indices]\n",
    "        lon_deg = lon_deg[indices]\n",
    "    \n",
    "    try:\n",
    "        lat = np.radians(lat_deg)\n",
    "        lon = np.radians(lon_deg)\n",
    "        # Use float32 to save memory\n",
    "        cos_ang = (np.cos(lat[:, None]).astype(np.float32) * np.cos(lat[None, :]).astype(np.float32) +\n",
    "                   np.sin(lat[:, None]).astype(np.float32) * np.sin(lat[None, :]).astype(np.float32) *\n",
    "                   np.cos((lon[:, None] - lon[None, :]).astype(np.float32)))\n",
    "        cos_ang = np.clip(cos_ang, -1.0, 1.0)\n",
    "        ang = np.arccos(cos_ang)\n",
    "        return R_EARTH * ang\n",
    "    except MemoryError:\n",
    "        print(f\"Memory error with {len(lat_deg)} points, subsampling further...\")\n",
    "        indices = np.random.choice(len(lat_deg), max_size//2, replace=False)\n",
    "        return safe_great_circle_matrix(lat_deg[indices], lon_deg[indices], max_size//2)\n",
    "\n",
    "def correlation_integral_from_dists(dists, N, r_vals):\n",
    "    \"\"\"Compute correlation integral C(r) from distances.\"\"\"\n",
    "    if len(dists) == 0 or N == 0:\n",
    "        return np.zeros_like(r_vals)\n",
    "    denom = max(1, N*(N-1))\n",
    "    C = np.zeros_like(r_vals)\n",
    "    for i, r in enumerate(r_vals):\n",
    "        try:\n",
    "            C[i] = 2.0 * np.sum(dists < r) / denom\n",
    "        except:\n",
    "            C[i] = 0.0\n",
    "    return C\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from list.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "# ---------------- Fractal Analysis ----------------\n",
    "def estimate_D2_from_positions_safe(lat, lon, n_r=N_R):\n",
    "    \"\"\"Estimate correlation dimension D2 safely from positions.\"\"\"\n",
    "    N = len(lat)\n",
    "    if N < 3:\n",
    "        return dict(D2=np.nan, D2_err=np.nan, r_min=np.nan, r_max=np.nan)\n",
    "    try:\n",
    "        Dmat = safe_great_circle_matrix(lat, lon)\n",
    "        iu = np.triu_indices(len(Dmat), k=1)\n",
    "        dists = Dmat[iu]\n",
    "        dists_pos = dists[dists > 0]\n",
    "        if dists_pos.size == 0:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=np.nan, r_max=np.nan)\n",
    "        r_min = np.min(dists_pos) * 1.2\n",
    "        r_max = np.max(dists_pos) / 2.0\n",
    "        if r_min <= 0 or r_min >= r_max:\n",
    "            r_min = np.min(dists_pos)\n",
    "            r_max = np.max(dists_pos)\n",
    "        if r_min >= r_max:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=r_min, r_max=r_max)\n",
    "        r_vals = np.logspace(math.log10(r_min), math.log10(r_max), n_r)\n",
    "        C = correlation_integral_from_dists(dists, len(Dmat), r_vals)\n",
    "        mask = C > 0\n",
    "        if mask.sum() < 6:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=r_min, r_max=r_max)\n",
    "        logr = np.log10(r_vals[mask])\n",
    "        logC = np.log10(C[mask])\n",
    "        i0 = len(logr)//4\n",
    "        i1 = 3*len(logr)//4\n",
    "        if i1 - i0 < 3:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=r_min, r_max=r_max)\n",
    "        slope, _, _, _, se = stats.linregress(logr[i0:i1], logC[i0:i1])\n",
    "        D2, D2_err = slope, se\n",
    "        fit_rmin, fit_rmax = 10**logr[i0], 10**logr[i1-1]\n",
    "        return dict(D2=D2, D2_err=D2_err, r_min=fit_rmin, r_max=fit_rmax)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in D2 estimation: {e}\")\n",
    "        return dict(D2=np.nan, D2_err=np.nan, r_min=np.nan, r_max=np.nan)\n",
    "\n",
    "# ---------------- B-Value ----------------\n",
    "def b_value_mle(mags, Mc):\n",
    "    \"\"\"Maximum likelihood estimate of b-value.\"\"\"\n",
    "    try:\n",
    "        mags = np.asarray(mags)\n",
    "        mags = mags[mags >= Mc]\n",
    "        if mags.size == 0: \n",
    "            return np.nan\n",
    "        Mbar = mags.mean()\n",
    "        if Mbar <= Mc: \n",
    "            return np.nan\n",
    "        return 0.4342944819 / (Mbar - Mc)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# ---------------- Window Processing ----------------\n",
    "def process_window_safe(idx_start, df, lat_col, lon_col, mag_col, win, Mc):\n",
    "    \"\"\"Process one sliding window safely.\"\"\"\n",
    "    try:\n",
    "        end_idx = min(idx_start + win, len(df))\n",
    "        subset = df.iloc[idx_start:end_idx]\n",
    "        if len(subset) < 3:\n",
    "            return None\n",
    "        lat = subset[lat_col].values\n",
    "        lon = subset[lon_col].values\n",
    "        mags = subset[mag_col].values\n",
    "        if np.any(np.isnan(lat)) or np.any(np.isnan(lon)):\n",
    "            return None\n",
    "        Dres = estimate_D2_from_positions_safe(lat, lon, n_r=N_R)\n",
    "        b = b_value_mle(mags, Mc)\n",
    "        D_pred = 2.3 - 0.73*b if (not np.isnan(b)) else np.nan\n",
    "\n",
    "        # Include original data for this window\n",
    "        subset_data = subset[['latitude','longitude','time']] if 'time' in subset.columns else subset[['latitude','longitude']]\n",
    "\n",
    "        return {\n",
    "            \"start_idx\": int(idx_start),\n",
    "            \"end_idx\": int(end_idx),\n",
    "            \"n_events\": int(len(subset)),\n",
    "            \"D2\": float(Dres[\"D2\"]) if not np.isnan(Dres[\"D2\"]) else np.nan,\n",
    "            \"D2_err\": float(Dres[\"D2_err\"]) if not np.isnan(Dres[\"D2_err\"]) else np.nan,\n",
    "            \"r_min_km\": float(Dres[\"r_min\"]) if not np.isnan(Dres[\"r_min\"]) else np.nan,\n",
    "            \"r_max_km\": float(Dres[\"r_max\"]) if not np.isnan(Dres[\"r_max\"]) else np.nan,\n",
    "            \"b\": float(b) if not np.isnan(b) else np.nan,\n",
    "            \"D_pred\": float(D_pred) if not np.isnan(D_pred) else np.nan,\n",
    "            \"events_data\": subset_data.to_dict(orient='records')  # Keep original lat/lon/time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing window {idx_start}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------- C(r) Analysis for Batch ----------------\n",
    "def analyze_Cr_for_batch_safe(batch_results, df, lat_col, lon_col, batch_idx):\n",
    "    \"\"\"Safe C(r) analysis for one batch.\"\"\"\n",
    "    try:\n",
    "        valid_results = [r for r in batch_results if r is not None]\n",
    "        if not valid_results:\n",
    "            return None\n",
    "        start_indices = [r['start_idx'] for r in valid_results]\n",
    "        end_indices = [r['end_idx'] for r in valid_results]\n",
    "        min_idx = min(start_indices)\n",
    "        max_idx = max(end_indices)\n",
    "        batch_events = df.iloc[min_idx:max_idx]\n",
    "        if len(batch_events) > MAX_EVENTS_FOR_CR:\n",
    "            indices = np.random.choice(len(batch_events), MAX_EVENTS_FOR_CR, replace=False)\n",
    "            batch_events = batch_events.iloc[indices]\n",
    "        if len(batch_events) < 10:\n",
    "            return None\n",
    "        lat = batch_events[lat_col].values\n",
    "        lon = batch_events[lon_col].values\n",
    "        if np.any(np.isnan(lat)) or np.any(np.isnan(lon)):\n",
    "            return None\n",
    "        Dmat = safe_great_circle_matrix(lat, lon)\n",
    "        iu = np.triu_indices(len(Dmat), k=1)\n",
    "        dists_km = Dmat[iu]\n",
    "        dists_deg = dists_km / 111.0\n",
    "        dists_deg = dists_deg[dists_deg > 0]\n",
    "        if len(dists_deg) < 10:\n",
    "            return None\n",
    "        r_min_deg = np.min(dists_deg) * 1.2\n",
    "        r_max_deg = np.max(dists_deg) / 2.0\n",
    "        if r_min_deg >= r_max_deg:\n",
    "            r_min_deg = np.min(dists_deg)\n",
    "            r_max_deg = np.max(dists_deg)\n",
    "        r_vals_deg = np.logspace(np.log10(r_min_deg), np.log10(r_max_deg), min(25, N_R))\n",
    "        N = len(lat)\n",
    "        C_r = correlation_integral_from_dists(dists_deg, N, r_vals_deg)\n",
    "        mask = C_r > 0\n",
    "        if mask.sum() < 6:\n",
    "            return None\n",
    "        r_vals_filtered = r_vals_deg[mask]\n",
    "        C_r_filtered = C_r[mask]\n",
    "        log_r = np.log10(r_vals_filtered)\n",
    "        log_C = np.log10(C_r_filtered)\n",
    "        start_idx = max(1, len(log_r)//4)\n",
    "        end_idx = min(len(log_r)-1, 3*len(log_r)//4)\n",
    "        if end_idx - start_idx < 3:\n",
    "            return None\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "            log_r[start_idx:end_idx], log_C[start_idx:end_idx]\n",
    "        )\n",
    "        return {\n",
    "            'batch_idx': batch_idx,\n",
    "            'n_events': len(batch_events),\n",
    "            'r_vals_deg': r_vals_filtered,\n",
    "            'C_r': C_r_filtered,\n",
    "            'slope': slope,\n",
    "            'intercept': intercept,\n",
    "            'r_value': r_value,\n",
    "            'p_value': p_value,\n",
    "            'std_err': std_err,\n",
    "            'fit_range': (10**log_r[start_idx], 10**log_r[end_idx-1])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in C(r) analysis for batch {batch_idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------- Safe Plotting Functions ----------------\n",
    "def plot_batch_Cr_safe(Cr_results, save_individual=True):\n",
    "    \"\"\"Plot C(r) and slope for all batches.\"\"\"\n",
    "    if not Cr_results:\n",
    "        print(\"No C(r) results to plot\")\n",
    "        return\n",
    "    try:\n",
    "        if save_individual:\n",
    "            for result in Cr_results[:10]:\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.loglog(result['r_vals_deg'], result['C_r'], 'bo-', markersize=4, linewidth=1, alpha=0.7)\n",
    "                log_r = np.log10(result['r_vals_deg'])\n",
    "                fit_line = 10**(result['slope'] * log_r + result['intercept'])\n",
    "                plt.loglog(result['r_vals_deg'], fit_line, 'r--', linewidth=2,\n",
    "                           label=f'Slope = {result[\"slope\"]:.3f} ± {result[\"std_err\"]:.3f}')\n",
    "                plt.xlabel('Distance r (degrees)')\n",
    "                plt.ylabel('Correlation Integral C(r)')\n",
    "                plt.title(f'Batch {result[\"batch_idx\"]}: C(r) vs r (N={result[\"n_events\"]})')\n",
    "                plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
    "                plt.legend()\n",
    "                plt.savefig(f'Cr_plot_batch_{result[\"batch_idx\"]:03d}.png', dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        # Comparison plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = plt.cm.viridis(np.linspace(0,1,len(Cr_results)))\n",
    "        for i, result in enumerate(Cr_results):\n",
    "            plt.loglog(result['r_vals_deg'], result['C_r'], 'o-', color=colors[i], markersize=2, linewidth=1, alpha=0.7,\n",
    "                       label=f'Batch {result[\"batch_idx\"]} (slope={result[\"slope\"]:.3f})')\n",
    "        plt.xlabel('Distance r (degrees)')\n",
    "        plt.ylabel('C(r)')\n",
    "        plt.title('C(r) Comparison All Batches')\n",
    "        plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "        if len(Cr_results) <= 10:\n",
    "            plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('all_batches.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        # Slope comparison\n",
    "        plt.figure(figsize=(10,6))\n",
    "        batch_indices = [r['batch_idx'] for r in Cr_results]\n",
    "        slopes = [r['slope'] for r in Cr_results]\n",
    "        errors = [r['std_err'] for r in Cr_results]\n",
    "        plt.errorbar(batch_indices, slopes, yerr=errors, fmt='bo-', capsize=3, markersize=4)\n",
    "        plt.xlabel('Batch Number')\n",
    "        plt.ylabel('C(r) Slope')\n",
    "        plt.title('Slope Comparison Across Batches')\n",
    "        plt.grid(True, alpha=0.7)\n",
    "        plt.axhline(np.mean(slopes), color='r', linestyle='--', label=f'Mean slope = {np.mean(slopes):.3f}')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('slope_comparison_batches.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"All plots saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting: {e}\")\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Reading CSV...\")\n",
    "        df = pd.read_csv(CSV_FILE)\n",
    "        print(f\"Loaded {len(df)} rows\")\n",
    "        \n",
    "        # Detect columns\n",
    "        lat_col = find_column(df, [\"latitude\",\"lat\",\"LAT\",\"Latitude\"])\n",
    "        lon_col = find_column(df, [\"longitude\",\"lon\",\"LON\",\"Longitude\",\"LONGITUDE\"])\n",
    "        mag_col = find_column(df, [\"magnitude\",\"mag\",\"MAG\",\"magnitude\"])\n",
    "        time_col = find_column(df, [\"time\",\"date\",\"datetime\",\"origin_time\"])\n",
    "        if lat_col is None or lon_col is None or mag_col is None:\n",
    "            raise RuntimeError(\"CSV missing latitude/longitude/magnitude columns.\")\n",
    "        print(f\"Using columns: {lat_col}, {lon_col}, {mag_col}, time={time_col}\")\n",
    "\n",
    "        # Optional depth filter\n",
    "        depth_col = find_column(df, [\"depth\",\"DEPTH\",\"depth_km\"])\n",
    "        if depth_col:\n",
    "            initial_len = len(df)\n",
    "            df = df[df[depth_col]<=60.0].reset_index(drop=True)\n",
    "            print(f\"Filtered by depth: {initial_len} -> {len(df)} events\")\n",
    "        \n",
    "        # Sort by time\n",
    "        if time_col:\n",
    "            df = df.sort_values(time_col).reset_index(drop=True)\n",
    "            print(\"Sorted by time\")\n",
    "\n",
    "        # Filter by magnitude\n",
    "        initial_len = len(df)\n",
    "        df = df[df[mag_col] >= MC].reset_index(drop=True)\n",
    "        n_total = len(df)\n",
    "        print(f\"Filtered by Mc={MC}: {initial_len} -> {n_total} events\")\n",
    "        if n_total < 10:\n",
    "            print(\"Too few events after filtering\")\n",
    "            return\n",
    "\n",
    "        # Set sliding window parameters\n",
    "        win = min(WINDOW_SIZE, max(10, n_total//10)) if n_total >= WINDOW_SIZE else max(10, n_total//3)\n",
    "        step = min(STEP, max(1, win//5)) if n_total >= WINDOW_SIZE else max(1, win//5)\n",
    "        start_indices = list(range(0, n_total - win + 1, step))\n",
    "        if MAX_WINDOWS:\n",
    "            start_indices = start_indices[:MAX_WINDOWS]\n",
    "        print(f\"Window size {win}, step {step}, total windows {len(start_indices)}\")\n",
    "        if len(start_indices)==0:\n",
    "            print(\"No windows to process\")\n",
    "            return\n",
    "\n",
    "        start_time = time.time()\n",
    "        worker = partial(process_window_safe, df=df, lat_col=lat_col, lon_col=lon_col, mag_col=mag_col, win=win, Mc=MC)\n",
    "\n",
    "        all_batches = list(chunks(start_indices, BATCH_SIZE))\n",
    "        batch_idx = 0\n",
    "        Cr_results = []\n",
    "\n",
    "        for batch in all_batches:\n",
    "            batch_idx += 1\n",
    "            print(f\"\\nProcessing batch {batch_idx}/{len(all_batches)} (size={len(batch)})\")\n",
    "            batch_results = []\n",
    "            try:\n",
    "                with Pool(processes=NUM_WORKERS) as pool:\n",
    "                    for res in tqdm(pool.imap_unordered(worker, batch), total=len(batch)):\n",
    "                        if res is not None:\n",
    "                            batch_results.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in multiprocessing for batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if not batch_results:\n",
    "                print(f\"No valid results for batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            # Save batch results including actual lat/lon/time\n",
    "            try:\n",
    "                # Flatten events_data for CSV\n",
    "                expanded_rows = []\n",
    "                for r in batch_results:\n",
    "                    for event in r['events_data']:\n",
    "                        row = r.copy()\n",
    "                        row.pop('events_data')\n",
    "                        row.update(event)\n",
    "                        expanded_rows.append(row)\n",
    "                batch_df = pd.DataFrame(expanded_rows)\n",
    "                batch_filename = f\"{OUT_BASE}_{batch_idx:03d}.csv\"\n",
    "                batch_df.to_csv(batch_filename, index=False)\n",
    "                print(f\"Saved {len(batch_df)} rows to {batch_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # C(r) analysis\n",
    "            print(f\"Analyzing C(r) for batch {batch_idx}...\")\n",
    "            Cr_result = analyze_Cr_for_batch_safe(batch_results, df, lat_col, lon_col, batch_idx)\n",
    "            if Cr_result:\n",
    "                Cr_results.append(Cr_result)\n",
    "                print(f\"Batch {batch_idx}: C(r) slope = {Cr_result['slope']:.4f} ± {Cr_result['std_err']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Batch {batch_idx}: Could not calculate C(r) slope\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nCompleted processing. Elapsed time: {elapsed:.2f} seconds\")\n",
    "\n",
    "        if Cr_results:\n",
    "            print(f\"\\nCreating C(r) plots for {len(Cr_results)} batches...\")\n",
    "            plot_batch_Cr_safe(Cr_results)\n",
    "            # Save summary CSV\n",
    "            try:\n",
    "                Cr_summary = pd.DataFrame([{\n",
    "                    'batch_idx': r['batch_idx'],\n",
    "                    'n_events': r['n_events'],\n",
    "                    'slope': r['slope'],\n",
    "                    'intercept': r['intercept'],\n",
    "                    'r_value': r['r_value'],\n",
    "                    'p_value': r['p_value'],\n",
    "                    'std_err': r['std_err'],\n",
    "                    'fit_r_min_deg': r['fit_range'][0],\n",
    "                    'fit_r_max_deg': r['fit_range'][1]\n",
    "                } for r in Cr_results])\n",
    "                Cr_summary.to_csv('Cr_slopes_summary.csv', index=False)\n",
    "                print(\"Saved C(r) slope analysis to 'Cr_slopes_summary.csv'\")\n",
    "                slopes = Cr_summary['slope'].values\n",
    "                print(f\"\\nC(r) Slope Statistics:\\nNumber of batches: {len(slopes)}\\nMean: {np.mean(slopes):.4f}\\nStd:  {np.std(slopes):.4f}\\nMin:  {np.min(slopes):.4f}\\nMax:  {np.max(slopes):.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving summary: {e}\")\n",
    "        else:\n",
    "            print(\"No valid C(r) results obtained.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ---------------- Entry ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0581ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 396271 rows from data.csv\n",
      "Events after Mc=3.0 filter: 127061\n",
      "Total windows to process: 5000 (window size 100, step 10)\n",
      "\n",
      "Processing batch 1/5 (windows in batch: 1000)\n",
      "Saved batch 1 -> final-data_001.csv (100000 rows / 1000 windows)\n",
      "\n",
      "Processing batch 2/5 (windows in batch: 1000)\n",
      "Saved batch 2 -> final-data_002.csv (100000 rows / 1000 windows)\n",
      "\n",
      "Processing batch 3/5 (windows in batch: 1000)\n",
      "Saved batch 3 -> final-data_003.csv (100000 rows / 1000 windows)\n",
      "\n",
      "Processing batch 4/5 (windows in batch: 1000)\n",
      "Saved batch 4 -> final-data_004.csv (100000 rows / 1000 windows)\n",
      "\n",
      "Processing batch 5/5 (windows in batch: 1000)\n",
      "Saved batch 5 -> final-data_005.csv (100000 rows / 1000 windows)\n",
      "\n",
      "Saved summary to Cr_slopes_summary.csv (rows: 5000)\n",
      "D2 stats -> count: 5000, mean: 1.2444, std: 0.2919, min: 0.2732, max: 1.9951\n"
     ]
    }
   ],
   "source": [
    "# script.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------- USER SETTINGS ----------------\n",
    "CSV_FILE = \"data.csv\"        # Input CSV (assumes columns: date,time,latitude,longitude,depth,magnitude)\n",
    "OUT_BASE = \"final-data\"      # Output base name\n",
    "MC = 3.0                     # Completeness magnitude (filter)\n",
    "WINDOW_SIZE = 100            # events per sliding window (like Hirata's N=100)\n",
    "STEP = 10                    # sliding step\n",
    "BATCH_SIZE = 1000            # how many windows per output CSV\n",
    "MAX_WINDOWS = 5000           # limit number of windows\n",
    "NUM_WORKERS = max(1, min(4, cpu_count()-1))\n",
    "N_R = 30                     # points in r for C(r) (logspace)\n",
    "MAX_DIST_SUBSAMPLE = 2000    # safe subsample if too many events for distance matrix\n",
    "R_EARTH = 6371.0             # km\n",
    "# ---------------- end settings ----------------\n",
    "\n",
    "def b_value_mle(mags, Mc):\n",
    "    mags = np.asarray(mags)\n",
    "    mags = mags[mags >= Mc]\n",
    "    if mags.size == 0:\n",
    "        return np.nan\n",
    "    Mbar = mags.mean()\n",
    "    if Mbar <= Mc:\n",
    "        return np.nan\n",
    "    return 0.4342944819 / (Mbar - Mc)  # ln10 / (Mbar - Mc)\n",
    "\n",
    "def safe_great_circle_matrix(lat_deg, lon_deg, max_size=MAX_DIST_SUBSAMPLE):\n",
    "    # Subsample if necessary to avoid huge memory\n",
    "    n = len(lat_deg)\n",
    "    if n > max_size:\n",
    "        idx = np.random.choice(n, max_size, replace=False)\n",
    "        lat_deg = lat_deg[idx]\n",
    "        lon_deg = lon_deg[idx]\n",
    "        n = max_size\n",
    "    lat = np.radians(lat_deg)\n",
    "    lon = np.radians(lon_deg)\n",
    "    # vectorized great-circle using cos law\n",
    "    cos_ang = (np.cos(lat)[:,None] * np.cos(lat)[None,:] +\n",
    "               np.sin(lat)[:,None] * np.sin(lat)[None,:] * np.cos((lon[:,None] - lon[None,:])))\n",
    "    cos_ang = np.clip(cos_ang, -1.0, 1.0)\n",
    "    ang = np.arccos(cos_ang)\n",
    "    return R_EARTH * ang  # distances in km\n",
    "\n",
    "def correlation_integral_from_dists(dists, N, r_vals):\n",
    "    # dists: 1D array of pairwise distances (same units as r_vals)\n",
    "    if len(dists)==0 or N < 2:\n",
    "        return np.zeros_like(r_vals)\n",
    "    denom = max(1, N*(N-1))\n",
    "    C = np.zeros_like(r_vals, dtype=float)\n",
    "    for i, r in enumerate(r_vals):\n",
    "        C[i] = 2.0 * np.sum(dists < r) / denom\n",
    "    return C\n",
    "\n",
    "def estimate_D2_from_positions(lat, lon, n_r=N_R):\n",
    "    \"\"\"\n",
    "    Compute D2 (correlation dimension) from lat/lon arrays following Hirata (1989).\n",
    "    Returns dict with D2, D2_err, r_min (deg), r_max (deg) where fit was done.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        N = len(lat)\n",
    "        if N < 3:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=np.nan, r_max=np.nan)\n",
    "\n",
    "        # compute pairwise distances (km), safe subsampling inside function\n",
    "        Dmat_km = safe_great_circle_matrix(lat, lon)\n",
    "        iu = np.triu_indices(len(Dmat_km), k=1)\n",
    "        dists_km = Dmat_km[iu]\n",
    "        # convert to degrees roughly (1 deg ~ 111 km)\n",
    "        dists_deg = dists_km / 111.0\n",
    "        dists_deg = dists_deg[dists_deg > 0]\n",
    "        if dists_deg.size < 10:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=np.nan, r_max=np.nan)\n",
    "\n",
    "        # r_min and r_max as Hirata: r_min = min(dists)*1.2, r_max = max(dists)/2\n",
    "        r_min = np.min(dists_deg) * 1.2\n",
    "        r_max = np.max(dists_deg) / 2.0\n",
    "        if r_min <= 0 or r_min >= r_max:\n",
    "            r_min = np.min(dists_deg)\n",
    "            r_max = np.max(dists_deg)\n",
    "        if r_min >= r_max:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=r_min, r_max=r_max)\n",
    "\n",
    "        r_vals = np.logspace(math.log10(r_min), math.log10(r_max), n_r)\n",
    "        C = correlation_integral_from_dists(dists_deg, len(Dmat_km), r_vals)\n",
    "        mask = C > 0\n",
    "        if mask.sum() < 6:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=r_min, r_max=r_max)\n",
    "\n",
    "        logr = np.log10(r_vals[mask])\n",
    "        logC = np.log10(C[mask])\n",
    "\n",
    "        # use middle 50% of points (i0..i1) as Hirata\n",
    "        L = len(logr)\n",
    "        i0 = L // 4\n",
    "        i1 = 3 * L // 4\n",
    "        if i1 - i0 < 3:\n",
    "            return dict(D2=np.nan, D2_err=np.nan, r_min=r_min, r_max=r_max)\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(logr[i0:i1], logC[i0:i1])\n",
    "        fit_rmin = 10**(logr[i0])\n",
    "        fit_rmax = 10**(logr[i1-1])\n",
    "        return dict(D2=float(slope), D2_err=float(std_err), r_min=fit_rmin, r_max=fit_rmax)\n",
    "    except Exception as e:\n",
    "        # on any error, return NaNs\n",
    "        return dict(D2=np.nan, D2_err=np.nan, r_min=np.nan, r_max=np.nan)\n",
    "\n",
    "def process_window(idx_start, df, win, Mc):\n",
    "    end_idx = min(idx_start + win, len(df))\n",
    "    subset = df.iloc[idx_start:end_idx]\n",
    "    if len(subset) < 3:\n",
    "        return None\n",
    "\n",
    "    mags = subset[\"magnitude\"].values\n",
    "    b = b_value_mle(mags, Mc)\n",
    "\n",
    "    lat = subset[\"latitude\"].values\n",
    "    lon = subset[\"longitude\"].values\n",
    "    Dres = estimate_D2_from_positions(lat, lon, n_r=N_R)\n",
    "\n",
    "    result = {\n",
    "        \"start_idx\": int(idx_start),\n",
    "        \"end_idx\": int(end_idx),\n",
    "        \"n_events\": int(len(subset)),\n",
    "        \"b_value\": float(b) if not np.isnan(b) else np.nan,\n",
    "        \"D2\": float(Dres[\"D2\"]) if not np.isnan(Dres[\"D2\"]) else np.nan,\n",
    "        \"D2_err\": float(Dres[\"D2_err\"]) if not np.isnan(Dres[\"D2_err\"]) else np.nan,\n",
    "        \"r_fit_min_deg\": float(Dres[\"r_min\"]) if not np.isnan(Dres[\"r_min\"]) else np.nan,\n",
    "        \"r_fit_max_deg\": float(Dres[\"r_max\"]) if not np.isnan(Dres[\"r_max\"]) else np.nan,\n",
    "        \"events_data\": subset[[\"date\",\"time\",\"latitude\",\"longitude\",\"magnitude\"]].to_dict(orient='records')\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    print(f\"Loaded {len(df)} rows from {CSV_FILE}\")\n",
    "\n",
    "    # filter by magnitude (Mc)\n",
    "    df = df[df[\"magnitude\"] >= MC].reset_index(drop=True)\n",
    "    n_total = len(df)\n",
    "    print(f\"Events after Mc={MC} filter: {n_total}\")\n",
    "    if n_total < 10:\n",
    "        print(\"Too few events after filtering; exiting.\")\n",
    "        return\n",
    "\n",
    "    # sliding windows\n",
    "    start_indices = list(range(0, n_total - WINDOW_SIZE + 1, STEP))\n",
    "    start_indices = start_indices[:MAX_WINDOWS]\n",
    "    print(f\"Total windows to process: {len(start_indices)} (window size {WINDOW_SIZE}, step {STEP})\")\n",
    "    if len(start_indices) == 0:\n",
    "        print(\"No windows to process; exit.\")\n",
    "        return\n",
    "\n",
    "    worker = partial(process_window, df=df, win=WINDOW_SIZE, Mc=MC)\n",
    "    all_batches = list(chunks(start_indices, BATCH_SIZE))\n",
    "\n",
    "    batch_idx = 0\n",
    "    Cr_results = []\n",
    "    for batch in all_batches:\n",
    "        batch_idx += 1\n",
    "        print(f\"\\nProcessing batch {batch_idx}/{len(all_batches)} (windows in batch: {len(batch)})\")\n",
    "        batch_results = []\n",
    "        try:\n",
    "            with Pool(processes=NUM_WORKERS) as pool:\n",
    "                for res in pool.imap_unordered(worker, batch):\n",
    "                    if res is not None:\n",
    "                        batch_results.append(res)\n",
    "        except Exception as e:\n",
    "            print(f\"Multiprocessing error: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not batch_results:\n",
    "            print(\"No valid windows in this batch.\")\n",
    "            continue\n",
    "\n",
    "        # Flatten and save batch CSV\n",
    "        expanded_rows = []\n",
    "        for r in batch_results:\n",
    "            base = {\n",
    "                \"start_idx\": r[\"start_idx\"],\n",
    "                \"end_idx\": r[\"end_idx\"],\n",
    "                \"n_events\": r[\"n_events\"],\n",
    "                \"b_value\": r[\"b_value\"],\n",
    "                \"D2\": r[\"D2\"],\n",
    "                \"D2_err\": r[\"D2_err\"],\n",
    "                \"r_fit_min_deg\": r[\"r_fit_min_deg\"],\n",
    "                \"r_fit_max_deg\": r[\"r_fit_max_deg\"]\n",
    "            }\n",
    "            for ev in r[\"events_data\"]:\n",
    "                row = base.copy()\n",
    "                row.update(ev)  # adds date,time,latitude,longitude,magnitude\n",
    "                expanded_rows.append(row)\n",
    "\n",
    "        batch_df = pd.DataFrame(expanded_rows)\n",
    "        out_file = f\"{OUT_BASE}_{batch_idx:03d}.csv\"\n",
    "        batch_df.to_csv(out_file, index=False)\n",
    "        print(f\"Saved batch {batch_idx} -> {out_file} ({len(batch_df)} rows / {len(batch_results)} windows)\")\n",
    "\n",
    "        # collect C(r)/D2 summary for later overall summary\n",
    "        for r in batch_results:\n",
    "            Cr_results.append({\n",
    "                \"batch_idx\": batch_idx,\n",
    "                \"start_idx\": r[\"start_idx\"],\n",
    "                \"end_idx\": r[\"end_idx\"],\n",
    "                \"n_events\": r[\"n_events\"],\n",
    "                \"b_value\": r[\"b_value\"],\n",
    "                \"D2\": r[\"D2\"],\n",
    "                \"D2_err\": r[\"D2_err\"],\n",
    "                \"r_fit_min_deg\": r[\"r_fit_min_deg\"],\n",
    "                \"r_fit_max_deg\": r[\"r_fit_max_deg\"]\n",
    "            })\n",
    "\n",
    "    # save summary if we have any results\n",
    "    if Cr_results:\n",
    "        summary_df = pd.DataFrame(Cr_results)\n",
    "        summary_file = \"Cr_slopes_summary.csv\"\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        print(f\"\\nSaved summary to {summary_file} (rows: {len(summary_df)})\")\n",
    "        # print basic stats\n",
    "        slopes = summary_df['D2'].dropna().values\n",
    "        if slopes.size > 0:\n",
    "            print(f\"D2 stats -> count: {len(slopes)}, mean: {np.mean(slopes):.4f}, std: {np.std(slopes):.4f}, min: {np.min(slopes):.4f}, max: {np.max(slopes):.4f}\")\n",
    "    else:\n",
    "        print(\"No valid D2/b-value results computed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ed3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
