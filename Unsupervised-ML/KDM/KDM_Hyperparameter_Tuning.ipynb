{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# KDM \u2014 SOM Hyperparameter Tuning\n### Grid Size \u00b7 Iterations \u00b7 Samples\n\nBased on **Section 3.2.3** of Septier et al. \u2014 the paper tunes three hyperparameters in sequence:\n1. **Grid size** (number of nodes) \u2014 scanned first, others fixed at max  \n2. **Number of iterations** \u2014 scanned after optimal grid found  \n3. **Number of training samples** \u2014 scanned last\n\nTwo scoring metrics guide the search:\n- **TE** (Topological Error) \u2192 measures topology preservation \u2192 target \u2248 0  \n- **QE** (Quantisation Error) \u2192 measures map resolution \u2192 target small  \n\nAdditionally, **classification performance** (balanced accuracy) is measured to find the grid size that best separates crisis vs non-crisis events.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0 \u00b7 Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n# \u2551              HYPERPARAMETER TUNING CONFIG            \u2551\n# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nINPUT_CSV    = \"your_catalogue.csv\"   # \u2190 your catalogue\nCSV_SEP      = \",\"\nFEATURE_COLS = [\"N+\", \"T+\", \"R+\", \"dm+\"]\nOUTPUT_DIR   = \"kdm_tuning\"\n\n# \u2500\u2500 Search grids \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGRID_SIZES   = [2, 3, 4, 5, 6, 8, 10]     # grid side lengths to test (2\u21924 nodes \u2026 10\u2192100)\nITER_VALUES  = [1_000, 5_000, 10_000, 25_000, 50_000, 100_000]   # iterations to test\nSAMPLE_FRACS = [0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0]  # fraction of data as training samples\n\n# \u2500\u2500 Fixed values while tuning the other param \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nFIXED_GRID_SIZE   = 4        # used while tuning iterations & samples\nFIXED_N_ITER      = 50_000   # used while tuning grid size & samples\nFIXED_SAMPLE_FRAC = 1.0      # used while tuning grid size & iterations\n\n# \u2500\u2500 Repeatability \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nN_REPEATS    = 3             # repeat each config N times (variance estimate)\nRANDOM_SEEDS = [42, 7, 123]  # one seed per repeat\n\n# \u2500\u2500 Convergence check (ratio stability) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Paper: crisis/non-crisis ratio should not vary > 5% across repeats\nRATIO_TOLERANCE = 0.05\n\nprint(\"\u2713 Config loaded\")\nprint(f\"  Grid sizes   : {GRID_SIZES}\")\nprint(f\"  Iterations   : {ITER_VALUES}\")\nprint(f\"  Sample fracs : {SAMPLE_FRACS}\")\nprint(f\"  Repeats      : {N_REPEATS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1 \u00b7 Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, time, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import RobustScaler\n\nwarnings.filterwarnings(\"ignore\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nplt.rcParams.update({\n    \"figure.dpi\": 130, \"axes.spines.top\": False,\n    \"axes.spines.right\": False, \"axes.grid\": True, \"grid.alpha\": 0.3,\n})\nprint(\"\u2713 Libraries ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2 \u00b7 SOM & Classifier (compact versions for fast tuning)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SOM:\n    \"\"\"Minimal SOM for hyperparameter tuning \u2014 same algorithm as full KDM.\"\"\"\n\n    def __init__(self, grid_size, n_features, n_iterations,\n                 learning_rate=0.5, random_state=42):\n        self.G  = grid_size\n        self.nf = n_features\n        self.ni = n_iterations\n        self.lr0 = learning_rate\n        self.s0  = grid_size / 2.0\n        self.rng = np.random.default_rng(random_state)\n        self.W   = self.rng.uniform(0, 1, (grid_size * grid_size, n_features)).astype(np.float32)\n        xx, yy   = np.meshgrid(np.arange(grid_size), np.arange(grid_size))\n        self._xy = np.column_stack([xx.ravel(), yy.ravel()])\n\n    def _bmu(self, x):\n        d = self.W - x\n        return int(np.argmin(np.einsum(\"ij,ij->i\", d, d)))\n\n    def fit(self, X, n_samples=None):\n        X   = X.astype(np.float32)\n        n   = len(X)\n        tau = self.ni / np.log(self.s0 + 1e-9)\n        # Subsample if n_samples specified\n        if n_samples is not None and n_samples < n:\n            idx = self.rng.choice(n, n_samples, replace=False)\n            X   = X[idx]\n            n   = n_samples\n        for t in range(1, self.ni + 1):\n            sigma = self.s0  * np.exp(-t / tau)\n            lr    = self.lr0 * np.exp(-t / self.ni)\n            x     = X[self.rng.integers(0, n)]\n            bmu   = self._bmu(x)\n            br, bc = divmod(bmu, self.G)\n            d2 = np.sum((self._xy - [br, bc]) ** 2, axis=1)\n            h  = np.exp(-d2 / (2 * max(sigma, 1e-4) ** 2)).reshape(-1, 1)\n            self.W += lr * h * (x - self.W)\n        return self\n\n    def map_flat(self, X):\n        X = X.astype(np.float32)\n        out = np.empty(len(X), dtype=int)\n        for i, x in enumerate(X):\n            d = self.W - x\n            out[i] = int(np.argmin(np.einsum(\"ij,ij->i\", d, d)))\n        return out\n\n    def topological_error(self, X, n=1500):\n        X = X[:n].astype(np.float32)\n        err = 0\n        for x in X:\n            d    = self.W - x\n            ds   = np.einsum(\"ij,ij->i\", d, d)\n            top2 = np.argsort(ds)[:2]\n            b1   = np.array(divmod(top2[0], self.G))\n            b2   = np.array(divmod(top2[1], self.G))\n            if np.max(np.abs(b1 - b2)) > 1:\n                err += 1\n        return err / len(X)\n\n    def quantisation_error(self, X, n=1500):\n        X = X[:n].astype(np.float32)\n        tot = 0.0\n        for x in X:\n            d   = self.W - x\n            tot += float(np.sqrt(np.einsum(\"ij,ij->i\", d, d).min()))\n        return tot / len(X)\n\n\ndef classify_som(som, X_scaled, n_clusters=None):\n    \"\"\"Run agglomerative clustering + centroid classification on trained SOM.\"\"\"\n    G   = som.G\n    nc  = n_clusters if n_clusters else max(2, (G * G) // 2)\n    agg = AgglomerativeClustering(n_clusters=nc, linkage=\"ward\")\n    node_cl = agg.fit_predict(som.W)\n    bmu_flat = som.map_flat(X_scaled)\n    ev_cl    = node_cl[bmu_flat]\n    cl_ids   = np.unique(node_cl)\n\n    centroids = np.array([\n        X_scaled[ev_cl == cid].mean(axis=0) if (ev_cl == cid).sum() > 0\n        else som.W[node_cl == cid].mean(axis=0)\n        for cid in cl_ids\n    ])\n\n    # Identify feature roles\n    feat_names = FEATURE_COLS\n    def fi(keys):\n        for i, n in enumerate(feat_names):\n            if any(k.lower() in n.lower() for k in keys): return i\n        return None\n    iT = fi([\"t+\"]); iR = fi([\"r+\"]); iN = fi([\"n+\"]); idm = fi([\"dm+\"])\n\n    Ak = np.zeros(len(cl_ids)); Bk = np.zeros(len(cl_ids))\n    for k in range(len(cl_ids)):\n        Ck = centroids[k]; a = b = 0.0\n        if iT  is not None: T=centroids[:,iT];  a+=abs(np.min(T)-Ck[iT])/(abs(np.min(T))+1e-12); b+=abs(np.max(T)-Ck[iT])/(abs(np.max(T))+1e-12)\n        if iR  is not None: R=centroids[:,iR];  a+=abs(np.min(R)-Ck[iR])/(abs(np.min(R))+1e-12); b+=abs(np.max(R)-Ck[iR])/(abs(np.max(R))+1e-12)\n        if iN  is not None: N=centroids[:,iN];  a+=abs(np.max(N)-Ck[iN])/(abs(np.max(N))+1e-12); b+=abs(np.min(N)-Ck[iN])/(abs(np.min(N))+1e-12)\n        if idm is not None: a+=abs(1.0-Ck[idm]); b-=abs(1.0-Ck[idm])\n        Ak[k]=a; Bk[k]=b\n\n    exp_A = np.exp(Ak); exp_B = np.exp(Bk)\n    p_crisis = exp_A / (exp_A + exp_B)\n    cl_labels = (p_crisis >= 0.5).astype(int)\n    ev_labels = cl_labels[ev_cl]\n\n    crisis_ratio = ev_labels.mean()\n    return ev_labels, crisis_ratio\n\n\ndef balanced_accuracy(y_true, y_pred):\n    \"\"\"Balanced accuracy for imbalanced classes (Eq. 17 in paper).\"\"\"\n    tp = ((y_pred == 1) & (y_true == 1)).sum()\n    fn = ((y_pred == 0) & (y_true == 1)).sum()\n    tn = ((y_pred == 0) & (y_true == 0)).sum()\n    fp = ((y_pred == 1) & (y_true == 0)).sum()\n    sens = tp / (tp + fn + 1e-9)\n    spec = tn / (tn + fp + 1e-9)\n    return 0.5 * (sens + spec)\n\nprint(\"\u2713 SOM and classifier ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3 \u00b7 Load Data & Scale Features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = pd.read_csv(INPUT_CSV, sep=CSV_SEP, low_memory=False)\nprint(f\"Loaded: {len(df):,} events | columns: {list(df.columns)}\")\n\nmissing = [c for c in FEATURE_COLS if c not in df.columns]\nif missing:\n    raise ValueError(f\"Columns not found: {missing}\")\n\nX_raw = df[FEATURE_COLS].copy()\nX_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\nX_raw = X_raw.fillna(X_raw.median())\n\nscaler   = RobustScaler()\nX_scaled = scaler.fit_transform(X_raw.values).astype(np.float32)\nN_TOTAL  = len(X_scaled)\n\n# Ground truth (if available) \u2014 used for balanced accuracy\n# If you have a 'label' or 'true_label' column, set this:\nGT_COL = None   # e.g. \"true_label\"   \u2190 change if available\ny_true = df[GT_COL].values.astype(int) if GT_COL and GT_COL in df.columns else None\n\nif y_true is not None:\n    print(f\"\u2713 Ground truth found in '{GT_COL}' \u2014 balanced accuracy will be computed\")\nelse:\n    print(\"\u2139 No ground truth column \u2014 only TE/QE/ratio metrics will be computed\")\n\nprint(f\"\u2713 Feature matrix: {X_scaled.shape}  |  scaler: RobustScaler\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4 \u00b7 Single-Run Helper"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_one(grid_size, n_iter, n_samples, seed):\n    \"\"\"Train one SOM and return TE, QE, crisis_ratio, balanced_acc, elapsed.\"\"\"\n    t0  = time.time()\n    som = SOM(grid_size=grid_size, n_features=X_scaled.shape[1],\n              n_iterations=n_iter, random_state=seed)\n    som.fit(X_scaled, n_samples=n_samples)\n    te  = som.topological_error(X_scaled)\n    qe  = som.quantisation_error(X_scaled)\n    ev_labels, ratio = classify_som(som, X_scaled)\n    ba  = balanced_accuracy(y_true, ev_labels) if y_true is not None else np.nan\n    return dict(te=te, qe=qe, ratio=ratio, bal_acc=ba, elapsed=time.time()-t0)\n\nprint(\"\u2713 run_one() ready \u2014 quick sanity check:\")\nr = run_one(FIXED_GRID_SIZE, 5000, None, 42)\nprint(f\"  TE={r['te']:.4f}  QE={r['qe']:.4f}  crisis_ratio={r['ratio']:.3f}\"\n      f\"  bal_acc={r['bal_acc']:.3f}  time={r['elapsed']:.1f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5 \u00b7 Phase 1 \u2014 Tune Grid Size\n\nFix iterations and samples at maximum, scan grid sizes.  \nTarget: **local minimum of TE and QE**, and **maximum balanced accuracy** for the binary classification task."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"  PHASE 1 \u2014 Grid Size Scan\")\nprint(f\"  Fixed: n_iter={FIXED_N_ITER:,}  samples=ALL\")\nprint(\"=\" * 60)\n\nresults_grid = []\n\nfor gs in GRID_SIZES:\n    row_te=[]; row_qe=[]; row_ratio=[]; row_ba=[]; row_t=[]\n    for seed in RANDOM_SEEDS[:N_REPEATS]:\n        r = run_one(gs, FIXED_N_ITER, None, seed)\n        row_te.append(r[\"te\"]); row_qe.append(r[\"qe\"])\n        row_ratio.append(r[\"ratio\"]); row_ba.append(r[\"bal_acc\"])\n        row_t.append(r[\"elapsed\"])\n\n    ratio_std = np.std(row_ratio)\n    stable    = ratio_std <= RATIO_TOLERANCE\n    results_grid.append(dict(\n        grid_size=gs,\n        nodes=gs*gs,\n        te_mean=np.mean(row_te),   te_std=np.std(row_te),\n        qe_mean=np.mean(row_qe),   qe_std=np.std(row_qe),\n        ratio_mean=np.mean(row_ratio), ratio_std=ratio_std,\n        ba_mean=np.nanmean(row_ba), ba_std=np.nanstd(row_ba),\n        stable=stable,\n        time_mean=np.mean(row_t),\n    ))\n    flag = \"\u2713 stable\" if stable else f\"\u2717 ratio_std={ratio_std:.3f}\"\n    print(f\"  Grid {gs}\u00d7{gs} ({gs*gs:3d} nodes) | \"\n          f\"TE={np.mean(row_te):.4f}\u00b1{np.std(row_te):.4f}  \"\n          f\"QE={np.mean(row_qe):.4f}\u00b1{np.std(row_qe):.4f}  \"\n          f\"BA={np.nanmean(row_ba):.3f}  {flag}\")\n\ndf_grid = pd.DataFrame(results_grid)\nprint(\"\\nDone.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Plot Phase 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nfig.suptitle(\"Phase 1 \u2014 Grid Size Scan\", fontsize=13, fontweight=\"bold\")\n\nmetrics = [\n    (\"te_mean\",  \"te_std\",  \"Topological Error (TE)\", \"#e74c3c\", \"lower is better\"),\n    (\"qe_mean\",  \"qe_std\",  \"Quantisation Error (QE)\",\"#3498db\", \"lower is better\"),\n    (\"ba_mean\",  \"ba_std\",  \"Balanced Accuracy\",       \"#2ecc71\", \"higher is better\"),\n]\nfor ax, (m, s, title, color, note) in zip(axes, metrics):\n    ax.errorbar(df_grid[\"nodes\"], df_grid[m], yerr=df_grid[s],\n                marker=\"o\", color=color, lw=2, capsize=4, markersize=7)\n    ax.set_xlabel(\"Number of nodes (G\u00b2)\", fontsize=11)\n    ax.set_title(f\"{title}\\n({note})\", fontsize=11, fontweight=\"bold\")\n    ax.set_xticks(df_grid[\"nodes\"])\n    ax.set_xticklabels([f\"{g}\u00d7{g}\\n({g*g})\" for g in df_grid[\"grid_size\"]], fontsize=8)\n\n    # Mark minimum / maximum\n    if \"lower\" in note:\n        best_idx = df_grid[m].idxmin()\n    else:\n        best_idx = df_grid[m].idxmax()\n    ax.axvline(df_grid.loc[best_idx, \"nodes\"], color=color, ls=\"--\", alpha=0.5)\n    ax.scatter([df_grid.loc[best_idx, \"nodes\"]], [df_grid.loc[best_idx, m]],\n               s=120, color=color, zorder=5, marker=\"*\")\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/phase1_grid_size.png\", bbox_inches=\"tight\")\nplt.show()\n\n# Recommendation\nbest_te = df_grid.loc[df_grid[\"te_mean\"].idxmin(), \"grid_size\"]\nbest_ba = df_grid.loc[df_grid[\"ba_mean\"].idxmax(), \"grid_size\"]\nprint(f\"\\n  Best TE      \u2192 grid size {best_te}\u00d7{best_te} ({best_te**2} nodes)\")\nprint(f\"  Best Bal.Acc \u2192 grid size {best_ba}\u00d7{best_ba} ({best_ba**2} nodes)\")\nprint(f\"\\n  Paper recommendation: 4\u00d74 (16 nodes) for binary crisis/non-crisis task\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Set optimal grid size \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Use BA-optimal if ground truth available, else TE-optimal\nif y_true is not None:\n    OPTIMAL_GRID = int(df_grid.loc[df_grid[\"ba_mean\"].idxmax(), \"grid_size\"])\nelse:\n    OPTIMAL_GRID = int(df_grid.loc[df_grid[\"te_mean\"].idxmin(), \"grid_size\"])\n\nprint(f\"\u2192 OPTIMAL_GRID_SIZE set to: {OPTIMAL_GRID}\u00d7{OPTIMAL_GRID} ({OPTIMAL_GRID**2} nodes)\")\nprint(\"  (Override manually if desired \u2014 paper uses 4\u00d74)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6 \u00b7 Phase 2 \u2014 Tune Number of Iterations\n\nFix grid at the optimal size found in Phase 1. Scan iteration counts.  \nTarget: **TE and QE plateau** (flat growth) \u2014 more iterations beyond this point add no benefit."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"  PHASE 2 \u2014 Iteration Count Scan\")\nprint(f\"  Fixed: grid={OPTIMAL_GRID}\u00d7{OPTIMAL_GRID}  samples=ALL\")\nprint(\"=\" * 60)\n\nresults_iter = []\n\nfor n_iter in ITER_VALUES:\n    row_te=[]; row_qe=[]; row_ratio=[]; row_ba=[]; row_t=[]\n    for seed in RANDOM_SEEDS[:N_REPEATS]:\n        r = run_one(OPTIMAL_GRID, n_iter, None, seed)\n        row_te.append(r[\"te\"]); row_qe.append(r[\"qe\"])\n        row_ratio.append(r[\"ratio\"]); row_ba.append(r[\"bal_acc\"])\n        row_t.append(r[\"elapsed\"])\n\n    ratio_std = np.std(row_ratio)\n    results_iter.append(dict(\n        n_iter=n_iter,\n        te_mean=np.mean(row_te),   te_std=np.std(row_te),\n        qe_mean=np.mean(row_qe),   qe_std=np.std(row_qe),\n        ratio_mean=np.mean(row_ratio), ratio_std=ratio_std,\n        ba_mean=np.nanmean(row_ba), ba_std=np.nanstd(row_ba),\n        time_mean=np.mean(row_t),\n    ))\n    print(f\"  iter={n_iter:>8,} | TE={np.mean(row_te):.4f}\u00b1{np.std(row_te):.4f}  \"\n          f\"QE={np.mean(row_qe):.4f}\u00b1{np.std(row_qe):.4f}  \"\n          f\"BA={np.nanmean(row_ba):.3f}  ratio_std={ratio_std:.4f}  \"\n          f\"time={np.mean(row_t):.1f}s\")\n\ndf_iter = pd.DataFrame(results_iter)\nprint(\"\\nDone.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Plot Phase 2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(1, 4, figsize=(18, 4))\nfig.suptitle(f\"Phase 2 \u2014 Iteration Scan  (grid {OPTIMAL_GRID}\u00d7{OPTIMAL_GRID})\",\n             fontsize=13, fontweight=\"bold\")\n\nspecs = [\n    (\"te_mean\",    \"te_std\",    \"Topological Error (TE)\", \"#e74c3c\"),\n    (\"qe_mean\",    \"qe_std\",    \"Quantisation Error (QE)\",\"#3498db\"),\n    (\"ba_mean\",    \"ba_std\",    \"Balanced Accuracy\",       \"#2ecc71\"),\n    (\"ratio_std\",  None,        \"Crisis Ratio Std Dev\\n(target < 0.05)\", \"#9b59b6\"),\n]\nfor ax, (m, s, title, color) in zip(axes, specs):\n    vals = df_iter[m]\n    ax.plot(df_iter[\"n_iter\"], vals, marker=\"o\", color=color, lw=2, markersize=7)\n    if s is not None:\n        ax.fill_between(df_iter[\"n_iter\"], vals - df_iter[s], vals + df_iter[s],\n                        alpha=0.2, color=color)\n    if m == \"ratio_std\":\n        ax.axhline(RATIO_TOLERANCE, color=\"red\", ls=\"--\", lw=1.5, label=f\"tolerance={RATIO_TOLERANCE}\")\n        ax.legend(fontsize=9)\n    ax.set_xlabel(\"Iterations\", fontsize=11)\n    ax.set_title(title, fontsize=10, fontweight=\"bold\")\n    ax.set_xscale(\"log\")\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/phase2_iterations.png\", bbox_inches=\"tight\")\nplt.show()\n\n# \u2500\u2500 Detect plateau: where TE improvement < 5% of total range \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nte_range  = df_iter[\"te_mean\"].max() - df_iter[\"te_mean\"].min()\nte_deltas = df_iter[\"te_mean\"].diff().abs().fillna(999)\nplateau_mask = te_deltas < 0.05 * te_range\nif plateau_mask.any():\n    OPTIMAL_ITERS = int(df_iter.loc[plateau_mask.idxmax(), \"n_iter\"])\nelse:\n    OPTIMAL_ITERS = int(df_iter[\"n_iter\"].iloc[-1])\n\nprint(f\"\u2192 TE plateau detected at: {OPTIMAL_ITERS:,} iterations\")\n\n# Also check ratio stability\nstable_iters = df_iter.loc[df_iter[\"ratio_std\"] <= RATIO_TOLERANCE, \"n_iter\"]\nif len(stable_iters) > 0:\n    OPTIMAL_ITERS = max(OPTIMAL_ITERS, int(stable_iters.iloc[0]))\n    print(f\"  Ratio stability achieved at: {int(stable_iters.iloc[0]):,} iterations\")\n\nprint(f\"\\n\u2192 OPTIMAL_N_ITER set to: {OPTIMAL_ITERS:,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7 \u00b7 Phase 3 \u2014 Tune Number of Training Samples\n\nFix grid and iterations at optimal values. Scan training sample counts.  \nTarget: **minimum samples** where QE stops improving \u2014 reducing training time without losing accuracy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"  PHASE 3 \u2014 Training Sample Count Scan\")\nprint(f\"  Fixed: grid={OPTIMAL_GRID}\u00d7{OPTIMAL_GRID}  iter={OPTIMAL_ITERS:,}\")\nprint(\"=\" * 60)\n\nresults_samples = []\nsample_counts   = [max(100, int(f * N_TOTAL)) for f in SAMPLE_FRACS]\nsample_counts   = sorted(set(sample_counts))   # deduplicate\n\nfor n_samp in sample_counts:\n    row_te=[]; row_qe=[]; row_ratio=[]; row_ba=[]; row_t=[]\n    for seed in RANDOM_SEEDS[:N_REPEATS]:\n        r = run_one(OPTIMAL_GRID, OPTIMAL_ITERS, n_samp, seed)\n        row_te.append(r[\"te\"]); row_qe.append(r[\"qe\"])\n        row_ratio.append(r[\"ratio\"]); row_ba.append(r[\"bal_acc\"])\n        row_t.append(r[\"elapsed\"])\n\n    ratio_std = np.std(row_ratio)\n    results_samples.append(dict(\n        n_samples=n_samp,\n        frac=n_samp/N_TOTAL,\n        te_mean=np.mean(row_te),   te_std=np.std(row_te),\n        qe_mean=np.mean(row_qe),   qe_std=np.std(row_qe),\n        ratio_mean=np.mean(row_ratio), ratio_std=ratio_std,\n        ba_mean=np.nanmean(row_ba), ba_std=np.nanstd(row_ba),\n        time_mean=np.mean(row_t),\n    ))\n    print(f\"  samples={n_samp:>7,} ({n_samp/N_TOTAL*100:4.0f}%) | \"\n          f\"TE={np.mean(row_te):.4f}\u00b1{np.std(row_te):.4f}  \"\n          f\"QE={np.mean(row_qe):.4f}\u00b1{np.std(row_qe):.4f}  \"\n          f\"ratio_std={ratio_std:.4f}  time={np.mean(row_t):.1f}s\")\n\ndf_samples = pd.DataFrame(results_samples)\nprint(\"\\nDone.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Plot Phase 3 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(1, 4, figsize=(18, 4))\nfig.suptitle(f\"Phase 3 \u2014 Sample Count Scan  (grid {OPTIMAL_GRID}\u00d7{OPTIMAL_GRID}, iter {OPTIMAL_ITERS:,})\",\n             fontsize=12, fontweight=\"bold\")\n\nspecs = [\n    (\"te_mean\",   \"te_std\",   \"Topological Error (TE)\", \"#e74c3c\"),\n    (\"qe_mean\",   \"qe_std\",   \"Quantisation Error (QE)\",\"#3498db\"),\n    (\"ba_mean\",   \"ba_std\",   \"Balanced Accuracy\",       \"#2ecc71\"),\n    (\"ratio_std\", None,       \"Crisis Ratio Std Dev\\n(target < 0.05)\", \"#9b59b6\"),\n]\nfor ax, (m, s, title, color) in zip(axes, specs):\n    vals = df_samples[m]\n    ax.plot(df_samples[\"n_samples\"], vals, marker=\"o\", color=color, lw=2, markersize=7)\n    if s is not None:\n        ax.fill_between(df_samples[\"n_samples\"], vals - df_samples[s], vals + df_samples[s],\n                        alpha=0.2, color=color)\n    if m == \"ratio_std\":\n        ax.axhline(RATIO_TOLERANCE, color=\"red\", ls=\"--\", lw=1.5, label=f\"tolerance={RATIO_TOLERANCE}\")\n        ax.legend(fontsize=9)\n    ax.set_xlabel(\"Training samples\", fontsize=11)\n    ax.set_title(title, fontsize=10, fontweight=\"bold\")\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/phase3_samples.png\", bbox_inches=\"tight\")\nplt.show()\n\n# \u2500\u2500 Optimal sample count: first n where ratio_std < tolerance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nstable = df_samples.loc[df_samples[\"ratio_std\"] <= RATIO_TOLERANCE, \"n_samples\"]\nif len(stable) > 0:\n    OPTIMAL_SAMPLES = int(stable.iloc[0])\nelse:\n    OPTIMAL_SAMPLES = int(df_samples[\"n_samples\"].iloc[-1])\n    print(f\"  \u26a0 Ratio never stabilised \u2014 using all samples\")\n\n# Also: find where QE plateaus\nqe_range  = df_samples[\"qe_mean\"].max() - df_samples[\"qe_mean\"].min()\nqe_deltas = df_samples[\"qe_mean\"].diff().abs().fillna(999)\nqe_plateau = df_samples.loc[qe_deltas < 0.05 * qe_range, \"n_samples\"]\nif len(qe_plateau) > 0:\n    qe_stable_n = int(qe_plateau.iloc[0])\n    OPTIMAL_SAMPLES = max(OPTIMAL_SAMPLES, qe_stable_n)\n\nprint(f\"\\n\u2192 OPTIMAL_SAMPLES set to: {OPTIMAL_SAMPLES:,} ({100*OPTIMAL_SAMPLES/N_TOTAL:.0f}% of data)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8 \u00b7 Summary \u2014 Optimal Hyperparameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\nprint(\"\u2551          OPTIMAL HYPERPARAMETERS (KDM)               \u2551\")\nprint(\"\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\")\nprint(f\"\u2551  Grid size       : {OPTIMAL_GRID}\u00d7{OPTIMAL_GRID} = {OPTIMAL_GRID**2} nodes{'':<28}\u2551\")\nprint(f\"\u2551  N iterations    : {OPTIMAL_ITERS:<34,}\u2551\")\nprint(f\"\u2551  Training samples: {OPTIMAL_SAMPLES:<34,}\u2551\")\nprint(\"\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\")\nprint(\"\u2551  Paper values (for reference):                       \u2551\")\nprint(\"\u2551    Grid size  : 4\u00d74 = 16 nodes                       \u2551\")\nprint(\"\u2551    Iterations : 100,000 (2,000,000 for CAT4)         \u2551\")\nprint(\"\u2551    Samples    : 7,000                                 \u2551\")\nprint(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n\nprint(\"\\nUse these values in your KDM notebook (Section 0 Config):\")\nprint(f\"  GRID_SIZE    = {OPTIMAL_GRID}\")\nprint(f\"  N_ITERATIONS = {OPTIMAL_ITERS}\")\nprint(f\"  # Training samples \u2248 {OPTIMAL_SAMPLES} (set in SOM.fit() via n_samples arg)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Combined summary plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(3, 2, figsize=(13, 11))\nfig.suptitle(\"KDM \u2014 Hyperparameter Tuning Summary\", fontsize=14, fontweight=\"bold\")\n\n# Phase 1\nfor ax, (m, lbl, color) in zip(axes[0], [\n    (\"te_mean\", \"TE\", \"#e74c3c\"), (\"qe_mean\", \"QE\", \"#3498db\")]):\n    ax.errorbar(df_grid[\"nodes\"], df_grid[m], yerr=df_grid[m.replace(\"mean\",\"std\")],\n                marker=\"o\", color=color, lw=2, capsize=4, markersize=7)\n    ax.axvline(OPTIMAL_GRID**2, color=\"k\", ls=\"--\", lw=1.5, label=f\"Optimal: {OPTIMAL_GRID}\u00b2\")\n    ax.set_xlabel(\"Nodes (G\u00b2)\"); ax.set_title(f\"Phase 1 \u2014 {lbl} vs Grid Size\", fontweight=\"bold\")\n    ax.set_xticks(df_grid[\"nodes\"])\n    ax.set_xticklabels([f\"{g}\u00b2\\n={g*g}\" for g in df_grid[\"grid_size\"]], fontsize=8)\n    ax.legend(fontsize=9)\n\n# Phase 2\nfor ax, (m, lbl, color) in zip(axes[1], [\n    (\"te_mean\", \"TE\", \"#e74c3c\"), (\"qe_mean\", \"QE\", \"#3498db\")]):\n    ax.plot(df_iter[\"n_iter\"], df_iter[m], marker=\"o\", color=color, lw=2, markersize=7)\n    ax.fill_between(df_iter[\"n_iter\"],\n                    df_iter[m]-df_iter[m.replace(\"mean\",\"std\")],\n                    df_iter[m]+df_iter[m.replace(\"mean\",\"std\")], alpha=0.2, color=color)\n    ax.axvline(OPTIMAL_ITERS, color=\"k\", ls=\"--\", lw=1.5, label=f\"Optimal: {OPTIMAL_ITERS:,}\")\n    ax.set_xlabel(\"Iterations (log)\"); ax.set_xscale(\"log\")\n    ax.set_title(f\"Phase 2 \u2014 {lbl} vs Iterations\", fontweight=\"bold\"); ax.legend(fontsize=9)\n\n# Phase 3\nfor ax, (m, lbl, color) in zip(axes[2], [\n    (\"te_mean\", \"TE\", \"#e74c3c\"), (\"ratio_std\", \"Ratio Std\", \"#9b59b6\")]):\n    ax.plot(df_samples[\"n_samples\"], df_samples[m], marker=\"o\", color=color, lw=2, markersize=7)\n    ax.axvline(OPTIMAL_SAMPLES, color=\"k\", ls=\"--\", lw=1.5, label=f\"Optimal: {OPTIMAL_SAMPLES:,}\")\n    if m == \"ratio_std\":\n        ax.axhline(RATIO_TOLERANCE, color=\"red\", ls=\":\", lw=1.5, label=\"Tolerance\")\n    ax.set_xlabel(\"Training samples\")\n    ax.set_title(f\"Phase 3 \u2014 {lbl} vs Samples\", fontweight=\"bold\"); ax.legend(fontsize=9)\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/summary_tuning.png\", bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Save all results to CSV \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndf_grid.to_csv(f\"{OUTPUT_DIR}/tuning_phase1_grid.csv\", index=False)\ndf_iter.to_csv(f\"{OUTPUT_DIR}/tuning_phase2_iter.csv\", index=False)\ndf_samples.to_csv(f\"{OUTPUT_DIR}/tuning_phase3_samples.csv\", index=False)\n\nsummary = pd.DataFrame([{\n    \"optimal_grid_size\":   OPTIMAL_GRID,\n    \"optimal_n_iter\":      OPTIMAL_ITERS,\n    \"optimal_n_samples\":   OPTIMAL_SAMPLES,\n}])\nsummary.to_csv(f\"{OUTPUT_DIR}/optimal_hyperparameters.csv\", index=False)\n\nprint(\"Saved:\")\nfor f in sorted(os.listdir(OUTPUT_DIR)):\n    print(f\"  {f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tips\n\n| Situation | Action |\n|---|---|\n| Ratio never stabilises | Increase `ITER_VALUES` upper bound or add more data |\n| TE stays high for all grids | Increase `FIXED_N_ITER` in Phase 1 |\n| Tuning is slow | Reduce `N_REPEATS` to 1 for a quick scan, then confirm with 3 |\n| No ground truth | Rely on TE/QE plateau + ratio stability \u2014 paper's primary criteria |\n| Large catalogue (>100k) | Increase `SAMPLE_FRACS` denominator; start `GRID_SIZES` at `[3,4,5]` |"
  }
 ]
}